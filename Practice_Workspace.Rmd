---
title: "Practice_Workspace.Rmd"
author: "Jonah Merriam"
date: "8/10/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal of this Rmd file is to create a reproducible set of code that will allow future people to repeat this analysis on 16S metagenomic data received from MRDNA. 

```{r Cleaning data in excel}
##The data given to me from my professor was from MR.DNA and was in an excel sheet. This excel sheet had different pages for the levels of phylum down to species. The one change I made to the excel sheet before reading it into R was to delete the empty columns in between the samples and in between the first sample and the label. This can be done in R but it took less than a minute in excel so I did it that way.
```


```{r packages}
##The first step to working through these steps in R will be to download these packages and make sure they are ready to use
#install.packages("paletteer")
#install.packages("janitor")
#install.packages("kableExtra")
#install.packages("skimr")
#install.packages("here")
#install.packages("readxl")
#install.packages("tidyverse")
#install.packages("vegan")
#install.packages("reshape2")
#install.packages("knitr")
#install.packages("ggpubr")
#install.packages("shiny")
#install.packages("sqldf")
#install.packages("dplyr")
```

``` {r libraries}
library(vegan)
library(tidyverse)
library(readxl)
library(here)
library(skimr) 
library(kableExtra)
library(janitor)
library(paletteer)
library(reshape2)
library(knitr)
library(ggpubr)
library(shiny)
library(sqldf)
library(dplyr)
library(data.table)
```

```{r Data Loading}
phylum_long<- read_excel(here("Frost IBS Data","Phylum Long.xlsx"))
class_long<- read_excel(here("Frost IBS Data", "Class Long .xlsx"))
order_long<- read_excel(here("Frost IBS Data", "Order Long.xlsx"))
family_long<- read_excel(here("Frost IBS Data", "Family Long.xlsx"))
genus_long<- read_excel(here("Frost IBS Data", "Genus Long Alphabetical .xlsx"))
species_long<- read_excel(here("Frost IBS Data", "Species Long Alphabetical.xlsx"))

phylum_wide<- read_excel(here("Frost IBS Data", "Frost IBS Original Data.xlsx"))
class_wide<- read_excel(here("Frost IBS Data", "Class Wide.xlsx"))
order_wide<- read_excel(here("Frost IBS Data", "Order Wide.xlsx"))
family_wide<- read_excel(here("Frost IBS Data", "Family Wide.xlsx"))
genus_wide<- read_excel(here("Frost IBS Data", "Genus Wide.xlsx"))
species_wide<- read_excel(here("Frost IBS Data", "Species Wide.xlsx"))
```

```{r Cleaning Data}
options(scipen=999)
##Changes the scientific notation to normal numbers

##Cleaning the long form data
phylum_long_na<-read_excel((here("Frost IBS Data","Phylum Long.xlsx")), na="0")
##This code reads in the original data file and replaces all of the 0 values with NA which allows for easier cleaning

phylum_long_nona<-phylum_long_na[rowSums(is.na(phylum_long_na)) != ncol(phylum_long_na[3:3]), ]
##This will remove the rows of data that have only NA in the columns specified, in this case [3:3]

phylum_long_clean<- phylum_long_nona %>% clean_names() %>%  mutate(phylum = str_to_lower(phylum))
##This code will put the column titles into a standardized snake case and will change selected character columns to all lowercase so the data is easier to handle. 

##Cleaning wide data
order_long_na<-read_excel((here("Frost IBS Data","Order Wide.xlsx")), na="0")

order_long_nona<-order_long_na[rowSums(is.na(order_long_na)) != ncol(order_long_na[2:8]), ]

```

```{r Order Graph}
order_long<- read_excel(here("Frost IBS Data", "Order Long.xlsx"))

order_long_shortened<- order_long %>% filter(order_relative_abundance >= 0.5)

order_barchart_shortened<-ggplot(order_long_shortened, aes(y=order_relative_abundance,fill=order,x=name))+ geom_bar(stat='identity',color="black",aes(fill=order))+ scale_color_paletteer_d("awtools::bpalette")+ labs(title="", x="",y="Order Relative abundance above 0.5%")+theme_bw() + theme(legend.position="right",axis.text.x = element_text(angle=45, hjust=1,vjust=1,color="black"))

order_barchart_shortened
```

```{r Tables}
##Read in the long form data
phylum_long<- read_excel(here("Frost IBS Data", "Phylum Long.xlsx"))

##This table will sort the data for each individual into groups based on the phylum. It includes the mean, median, and standard deviation of each of the individuals relative abundances across each phylum.
phylum_summary_table <- phylum_long %>% group_by(phylum) %>% summarize(mean_abundance = mean(phylum_relative_abundance), median_abundance = median(phylum_relative_abundance), sd_relative_abundance = sd(phylum_relative_abundance))
phylum_summary_table

##The filter on this allows it to weed out the phylum's with less than 1% relative abundance which gives a better idea of the most prevalent phylum's 
phylum_table_above_1<- phylum_long %>% group_by(phylum) %>% filter(phylum_relative_abundance >= 1) %>% summarize(name = n(),mean_abundance = mean(phylum_relative_abundance), median_abundance = median(phylum_relative_abundance), sd_relative_abundance = sd(phylum_relative_abundance), shannon = (shannon_phylum))
kable(phylum_table_above_1)

```

```{r Diversity calculations}
##Calculating alpha diversity within each sample 


##Calculating Shannon with wide data set 

shannon_order<-diversity(order_wide[2:8],index="shannon",2)

shannon_phylum<-diversity(phylum_wide[2:8], index="shannon",2)

##Inverse Simpson
invsimp_phylum<-diversity(phylum_wide[2:8],index="invsimpson",2)

invsimp_order<-diversity(order_wide[2:8],index="invsimpson",2)

alpha_diversity<-data.frame(shannon_order, invsimp_order)

##Merging alpha data and long form data
alpha_diversity<-setDT(alpha_diversity, keep.rownames = TRUE)[]
alpha_diversity<- alpha_diversity %>% mutate(name=alpha_diversity$rn)
alpha_diversity <- alpha_diversity[,-(1),drop=FALSE]

alpha_order_melted<-melt(alpha_diversity)

##Making a graphical representation
ggplot(alpha_order_melted, aes(x=name, y=value, fill=variable,shape=variable))+geom_point(size=4,aes(color=variable))+facet_grid(variable~.,scales="free")+theme_bw()+theme(axis.text.x = element_text(angle = 90,hjust=1,vjust=0.5,size=8),axis.text.y=element_text(size=12),legend.position = "top")
```

```{r Shapiro Test}
##Will cause data to be expressed in decimals and not scientific notation 
options(scipen=999)

##This code will run the shapiro wilks test in groups based off of the taxonomic level. This allows the same level to be gathered across samples so that they can be tested for normality. Each order will receive a P value based on if it 
order_long_stat<- ddply(order_long,.(order), summarise, cbind(if(length(unique(order_relative_abundance))==1) NA else shapiro.test(order_relative_abundance)$p.value, if(length(unique(order_relative_abundance))==1) NA else shapiro.test(order_relative_abundance)$statistic))

##This will create a data frame with the pvalue of the shapiro test and the W-statistic. The pvalue will be the used to tell if the specific order is distributed normally by accepting sets as normally distributed if they are over the value of p>0.05
order_long_shapiro<-data.frame(order=order_long_stat[,1],as.data.frame(order_long_stat[,2]),stringsAsFactors=FALSE)
names(order_long_shapiro)[2:3]<-c("Pvalue","stats")

##Removing all tests with pvalue below 0.5
order_long_normally_distributed<- order_long_shapiro %>% filter(Pvalue >= 0.05)

##Combing pvalue and relative abundance  
order_agg = merge(order_long_normally_distributed, order_long, by="order")
```

```{r Individual Significance Data}
##Adding mean and sd to the data frame
order_agg_test<- order_agg %>% mutate(mean= ave(order_agg$order_relative_abundance, order_agg$order)) %>% mutate(sd=(ave(order_agg$order_relative_abundance, order_agg$order, FUN=sd)))

##Calculating Z scores
order_zscore %>% mutate(zscore = ((order_agg_test$order_relative_abundance - order_agg_test$mean)/sd))

order_zscore<- order_agg_test %>% mutate(zscore = ((order_agg_test$order_relative_abundance - order_agg_test$mean)/sd))

##Filtering for +/- 2 sd away from mean 
order_2sd <- order_zscore %>% filter(zscore >= 2) %>% filter(zscore >= -2)
```

```{r Stats for Species Level}
##Two step process of getting shapiro wilks results
species_long_stat<- ddply(species_long,.(species), summarise, cbind(if(length(unique(species_relative_abundance))==1) NA else shapiro.test(species_relative_abundance)$p.value, if(length(unique(species_relative_abundance))==1) NA else shapiro.test(species_relative_abundance)$statistic))

species_long_shapiro<- data.frame(species=species_long_stat[,1],as.data.frame(species_long_stat[,2]),stringsAsFactors=FALSE)
names(species_long_shapiro)[2:3]<-c("Pvalue","stats")

##Removing all tests with pvalue below 0.5
species_long_normally_distributed<- species_long_shapiro %>% filter(Pvalue >= 0.05)

##Combing pvalue and relative abundance  
species_agg = merge(species_long_normally_distributed, species_long, by="species")

##Adding mean and sd to the data frame
species_agg_test<- species_agg %>% mutate(mean= ave(species_agg$species_relative_abundance, species_agg$species)) %>% mutate(sd=(ave(species_agg$species_relative_abundance, species_agg$species, FUN=sd)))

##Calculating Z-scores
species_zscore<- species_agg_test %>% mutate(zscore = ((species_agg_test$species_relative_abundance - species_agg_test$mean)/sd))

##Filtering for +/-2sd
species_2sd <- species_zscore %>% filter(zscore >= 2 | zscore <= -2)

##Filtering for +/-1.5sd
species_1.5sd <- species_zscore %>% filter(zscore >=1.5 | zscore <= -1.5)

##Filtering for k_b_m
species_1.5sd_kbm<- species_1.5sd %>% filter(name=="k_b_m")
```

